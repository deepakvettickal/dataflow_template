Dataflow pipeline template to read data from a bigquery table, process it and push it to another bigquery table.
With cloud-build image+template building, artifacts registry, and dataflow=prime.

Steps to push logic to dataflow:  
    1. Define the pipeline in dataflow.py (define the DoMain class for parallel processing, and define pipeline options)
    2. Define the logic in app folder (example code in app folder is a bigquery-to-bigquery etl job).
    3. From root, 'run python deploy.py init' to create a GCP folder for dataflow files and setup Artifacts Registry for stroing docker images. 
    4. Run 'python deploy.py deploy' to push a job to dataflow.
    5. Use the template generated by the deploy script to setup a dataflow pipeline. 

Put all the logic in app folder and define pcollections in config.py for parallel processing.
Use the following commands to setup the pipeline on dataflow.

## dataflow-template  
-> Run "python deploy.py" from root to see available deploy commands.  
  
## DEPLOY CODE TO DATAFLOW  
To deploy code to etl-pipeline on Dataflow:  
    From root, run    
    --> "python deploy.py deploy True"  (To deploy both the container image and the dataflow template) (The last argument is the value of prod)   

The changes will be reflected in the next pipeline job.  
  
## TESTING  
To test if changes made works on Dataflow:  
    From root, run   
    --> "python deploy.py image"  (To build the container image and push it to Artifact Registry)    
    --> "python deploy.py job False"  (To start a job from local machine to Dataflow) (The last argument is the value of prod)
    --> "python deploy.py template True"  (To deploy the dataflow template) (The last argument is the value of prod)  

The new job can be found at Dataflow Jobs tab    
  
## RUN main.py  
To run the main function on its own, from ROOT run:  
    --> "python -m app.main"  
